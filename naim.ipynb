{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d7a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Process a single frame for person detection (tracking not included)\n",
    "def process_frame(frame, model):\n",
    "    #frame actually a picture\n",
    "    results = model(frame)\n",
    "    detections = []\n",
    "\n",
    "    for box in results[0].boxes:\n",
    "        x1, y1, x2, y2 = box.xyxy[0]\n",
    "        conf = float(box.conf)\n",
    "        cls = int(box.cls)\n",
    "\n",
    "        # class 0 = person in COCO\n",
    "        if cls == 0 and conf > 0.5:\n",
    "            detections.append([[float(x1), float(y1), float(x2 - x1), float(y2 - y1)],conf])\n",
    "    \n",
    "    return detections\n",
    "\n",
    "\n",
    "# now, detections is a list of list like [[x, y, w, h]] which is the information of bounding boxes around the person detected in the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779d9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 16 persons, 68.1ms\n",
      "Speed: 2.5ms preprocess, 68.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Detections: [[175.90618896484375, 283.386474609375, 59.803466796875, 140.6126708984375], [22.92767333984375, 247.980224609375, 55.08995819091797, 135.06640625], [336.8275146484375, 20.076690673828125, 44.910888671875, 122.70515441894531], [269.208740234375, 222.44500732421875, 54.98626708984375, 134.27813720703125], [459.22979736328125, 239.7570343017578, 64.21868896484375, 123.30662536621094], [235.919921875, 138.51385498046875, 42.9990234375, 115.30282592773438], [302.77349853515625, 117.91400146484375, 46.9053955078125, 122.28732299804688], [95.42851257324219, 94.6858901977539, 44.63526916503906, 118.70928192138672], [131.28701782226562, 134.75360107421875, 45.83489990234375, 117.2784423828125], [189.84017944335938, 82.3543701171875, 48.1597900390625, 165.18399047851562], [71.75164794921875, 6.6855316162109375, 34.08937072753906, 105.83248901367188], [164.82196044921875, 160.82415771484375, 57.8238525390625, 148.7620849609375]]\n"
     ]
    }
   ],
   "source": [
    "### validation of the process_frame function ###\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Function to process frame (detect persons only)\n",
    "def process_frame(frame, model):\n",
    "    results = model(frame)\n",
    "    detections = []\n",
    "\n",
    "    for box in results[0].boxes:\n",
    "        x1, y1, x2, y2 = box.xyxy[0]\n",
    "        conf = float(box.conf)\n",
    "        cls = int(box.cls)\n",
    "\n",
    "        # class 0 = person in COCO dataset\n",
    "        if cls == 0 and conf > 0.5:\n",
    "            detections.append([float(x1), float(y1), float(x2 - x1), float(y2 - y1)])  # [x, y, w, h]\n",
    "\n",
    "    return detections\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    image_path = \"20.jpg\"   \n",
    "    frame = cv2.imread(image_path)\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    if frame is None:\n",
    "        print(\"Error: Image not found!\")\n",
    "        return\n",
    "\n",
    "    detections = process_frame(frame, model)\n",
    "    print(\"Detections:\", detections)\n",
    "\n",
    "    # Draw green rectangles for each detection\n",
    "    for (x, y, w, h) in detections:\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Show the image\n",
    "    cv2.imshow(\"Person Detections\", frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f19f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "### anomalies detection function ###\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "def anomaly_detection(detections, frame):\n",
    "    def focal_loss(gamma=2., alpha=0.25):\n",
    "        def focal_loss_fixed(y_true, y_pred):\n",
    "            y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())  # numerical stability\n",
    "            cross_entropy = -y_true * K.log(y_pred)\n",
    "            loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "            return K.sum(loss, axis=1)\n",
    "        return focal_loss_fixed\n",
    "    \n",
    "    model = load_model('densenet_focal_epoch2.h5',custom_objects={'focal_loss_fixed': focal_loss(gamma=2.0, alpha=0.25)})\n",
    "    for (x, y, w, h) in detections:\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        cropped = frame[y:y+h, x:x+w]\n",
    "        cropped = cv2.resize(cropped, (64, 64))\n",
    "        cropped_array = image.img_to_array(cropped)\n",
    "        cropped_array = np.expand_dims(cropped_array, axis=0)  # Add batch dimension\n",
    "        cropped_array /= 255.0\n",
    "\n",
    "        prediction = model.predict(cropped_array)\n",
    "        prdiction_class = np.argmax(prediction, axis=1)[0]\n",
    "        if prdiction_class == 7:\n",
    "            return False\n",
    "        else:\n",
    "            return True  # Anomaly detected  \n",
    "    return False  # No anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6302b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import torchreid\n",
    "\n",
    "tracker = DeepSort(max_age=90,embedder=\"torchreid\",\n",
    "    embedder_model_name=\"resnet50\",  \n",
    "    half=True  )\n",
    "\n",
    "def tracking(detections, frame):\n",
    "    dets = []\n",
    "    for (x, y, w, h) in detections:\n",
    "        x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)\n",
    "        dets.append(([x1, y1, x2, y2], 1.0))  \n",
    "\n",
    "    tracks = tracker.update_tracks(dets, frame=frame)\n",
    "\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "        x1, y1, x2, y2 = map(int, ltrb)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, f'ID: {track_id}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f6c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d126046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078990f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee240c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
